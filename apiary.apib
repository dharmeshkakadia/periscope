FORMAT: 1A
# Periscope

![SequenceIQ](https://raw.githubusercontent.com/sequenceiq/sequenceiq.github.io/master/img/logo.png)


*Periscope is a powerful, fast, thick and top-to-bottom right-hander, eastward from Sumbawa's famous west-coast. Timing is critical, as needs a number of elements to align before it shows its true colors.*

*Periscope is a heuristic Hadoop scheduler you associate with a QoS profile. Built on YARN schedulers, cloud and VM resource management API's it allows you to associate SLAs to applications and customers.*

##Overview

The purpose of Periscope is to bring QoS to a multi-tenant Hadoop cluster, while allowing to apply SLAs to individual applications and customers.
At [SequenceIQ](http://sequenceiq.com) working with multi-tenant Hadoop clusters for quite a while we have always seen the same frustration and fight for resource between users.
The **FairScheduler** was partially solving this problem - bringing in fairness based on the notion of [Dominant Resource Fairness](http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf).
With the emergence of Hadoop 2 YARN and the **CapacityScheduler** we had the option to maximize throughput and utilization for a multi-tenant cluster in an operator-friendly manner.
The scheduler works around the concept of queues. These queues are typically setup by administrators to reflect the economics of the shared cluster.
While this is a pretty good abstraction and brings some level of SLA for `predictable` workloads, it often needs proper `design ahead`.
The queue hierarchy and resource allocation needs to be changed when new tenants and workloads are moved to the cluster.

Periscope was designed around the idea of `dynamic` clusters - without any need to preconfigure queues, cluster nodes or apply capacity planning ahead.

##How it works

Periscope monitors the application progress, the number of YARN containers/resources and their allocation, queue depths, and the number of available cluster nodes and their health.
Since we have switched to YARN a while ago (been among the first adopters) we have run an open source [monitoring project](https://github.com/sequenceiq/yarn-monitoring), based on R.
We have been collecting metrics from the YARN Timeline server, Hadoop Metrics2 and Ambari's Nagios/Ganglia - and profiling the applications and correlating with these metrics.
One of the key findings was that while low level metrics are good to understand the cluster health - they might not necessarily help on making decisions when applying different SLAs on a multi-tenant cluster. 
Focusing on higher level building blocks as queue depth, YARN containers, etc actually brings in the same quality of service, while not being lost in low level details.

_Example: Applying SLA based on `resource` load might not be the best solution - each application tasks generates different loads, and a CPU heavy step might be followed by an I/O heavy one - making a decision based on a low `snapshot` might not be the right option (the FairScheduler does something similar for the dominant resource).
Also note that a YARN cluster can run different applications - MR2, HBase, Spark, etc - and they all generate different load across different timeframes.
When YARN allocates containers it associates `resources` - it's actually more predictable to let YARN to deal with the resource allocation, and have Periscope orchestrate the process._

Periscope works with two types of Hadoop clusters: `static` and `dynamic`. Periscope does not require any pre-installation - the only thing it requires is to be `attached` to an Ambari server's REST API.

##Clusters

### Static clusters
From Periscope point of view we consider a cluster `static` when the cluster capacity can't be increased horizontally.
This means that the hardware resources are already given - and the throughput can't be increased by adding new nodes.
Periscope introspects the job submission process, monitors the applications and applies the following SLAs:

  1. Application ordering - can guaranty that a higher priority application finishes before another one (supporting parallel or sequential execution)
  2. Moves running applications between priority queues
  3. *Attempts* to enforce time based SLA (execution time, finish by, finish between, recurring)
  4. *Attempts* to enforce guaranteed cluster capacity requests ( x % of the resources)
  5. Support for distributed (but not YARN ready) applications using Apache Slider
  6. Attach priorities to SLAs

### Dynamic clusters
From Periscope point of view we consider a cluster `dynamic` when the cluster capacity can be increased horizontally.
This means that nodes can be added or removed dynamically - thus the throughput can be increased or decreased based on the cluster load, and scheduled applications.
In order to do that Periscope instructs [Cloudbreak](http://sequenceiq.com/cloudbreak/) to add or remove nodes from the cluster based on the SLAs and thus continuously provide a high *quality of service* for the multi-tenand Hadoop cluster.
Just to refresh memories - [Cloudbreak](http://sequenceiq.com/products.html) is [SequenceIQ's](http://sequenceiq.com) open source, cloud agnostic Hadoop as a Service API.
Given the option of provisioning or decommissioning cluster nodes on the fly, Periscope allows you to use the following set of SLAs:

  1. Application ordering - can guaranty that a higher priority application finishes before another one (supporting parallel or sequential execution)
  2. Moves running applications between priority queues
  3. *Enforce* time based SLA (execution time, finish by, finish between, recurring) by increasing cluster capacity and throughput
  4. Smart decommissioning - avoids HDFS storms, keeps `payed` nodes alive till the last minute
  5. *Enforce* guaranteed cluster capacity requests ( x % of the resources)
  6. *Private* cluster requests - supports provisioning of short lived private clusters with the possibility to merge
  7. Support for distributed (but not YARN ready) applications using Apache Slider
  8. Attach priorities to SLAs


#Group Clusters
A Hadoop cluster is a set of components and services launched in order to store, analyze and process unstructured data. 
Periscope can work with any Hadoop 2/ YARN cluster provisioned with Apache Ambari, and supports any YARN application.

##Add, remove or show [/clusters/{id}]
###Add a cluster [POST]
Add a Hadoop cluster to be monitored by Periscope. Note that Periscope does not require any pre-installation of components on cluster nodes. 
To link a Hadoop cluster with Periscope the only required this is the Apache Ambari REST API endpoint.

+ Parameters
    + id (required String `id`) ... The id of the cluster to be monitored.

+ Request (application/json)

        {
            "host": "172.24.0.2",
            "port": "8080",
            "user": "admin",
            "pass": "admin"
        }

+ Response 201 (application/json)

        {
            "appMovement": "allowed",
            "state": "RUNNING",
            "port": "8080",
            "host": "172.24.0.2",
            "id": "multi-node"
        }

###Retrieve cluster information [GET]

+ Parameters
    + id (required String `id`) ... The id of the cluster.

+ Request

        {

        }

+ Response 200 (application/json)

        {
            "appMovement": "allowed",
            "state": "RUNNING",
            "port": "8080",
            "host": "172.24.0.2",
            "id": "multi-node"
        }

###Remove a cluster [DELETE]

+ Parameters
    + id (required String `id`) ... The id of the cluster.

+ Request

        {

        }

+ Response 200 (application/json)

        {
            "appMovement": "allowed",
            "state": "RUNNING",
            "port": "8080",
            "host": "172.24.0.2",
            "id": "multi-node"
        }
    
##List [/clusters]
###Retrieve cluster information [GET]

+ Request

        {

        }

+ Response 200 (application/json)

        [
          {
            "appMovement": "allowed",
            "state": "RUNNING",
            "port": "8080",
            "host": "172.24.0.2",
            "id": "multi-node"
          }
        ]
        

##State [/clusters/{clusterId}/state]
###Set state [POST]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {
          "state": "SUSPENDED"
        }

+  Response 200 (application/json)

        {
            "appMovement": "prohibited",
            "state": "SUSPENDED",
            "port": "8080",
            "host": "172.24.0.2",
            "id": "multi-node"
        }

##Movements [/clusters/{clusterId}/movement]
###Configure application movements [POST]
Configure whether moveing the submitted applications between different queues to set a priority among them is allowed or not. 
Based on our contribution to the CapacityScheduler [YARN-2248](https://issues.apache.org/jira/browse/YARN-2248) we made available to move running applications between queues.

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {
          "allowed": "false" 
        }

+  Response 200 (application/json)

        {
            "appMovement": "prohibited",
            "state": "SUSPENDED",
            "port": "8080",
            "host": "172.24.0.2",
            "id": "multi-node"
        }

#Group Alarms
An alarm watches a metric over a specified time period, and used by one or more action or scaling policy based on the value of the metric relative to a given threshold over a number of time periods.
The current `metrics` are: 

*`PENDING_CONTAINERS`- pending YARN containers

*`PENDING_APPLICATIONS` - pending/queued YARN applications

*`LOST_NODES` - cluster nodes lost

*`UNHEALTHY_NODES` - unhealthy cluster nodes

*`GLOBAL_RESOURCES` - global resources 

The `comparison operators` are: `LESS_THAN`, `GREATER_THAN`, `LESS_OR_EQUAL_THAN`, `GREATER_OR_EQUAL_THAN`, `EQUALS`.

The `period`  parameter specifies the time period in minutes during the alarm has to be sustained. 
This allows operators to avoid reacting for sudden spikes in the system and apply policies only in case of a sustained system stress.

The `threshold` parameter specifies the variance applied by the `operator` for the selected `metric`.

##Add, remove and show [/clusters/{clusterId}/alarms]
###Create an alarm [POST]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {
          "alarms": [
            {
              "alarmName": "pendingContainerHigh",
              "description": "Number of pending containers is high",
              "metric": "PENDING_CONTAINERS",
              "threshold": 10,
              "comparisonOperator": "GREATER_THAN",
              "period": 10
            },
            {
              "alarmName": "freeGlobalResourcesRateLow",
              "description": "Low free global resource rate",
              "metric": "GLOBAL_RESOURCES",
              "threshold": 1,
              "comparisonOperator": "EQUALS",
              "period": 10
            }
          ]
        }

+ Response 201 (application/json)

        {
         "alarms": [
            {
             "scalingPolicyId": null,
             "period": 10,
             "comparisonOperator": "GREATER_THAN",
             "threshold": 10,
             "metric": "PENDING_CONTAINERS",
             "description": "Number of pending containers is high",
             "alarmName": "pendingContainerHigh",
             "id": 50
            },
            {
             "scalingPolicyId": null,
             "period": 10,
             "comparisonOperator": "EQUALS",
             "threshold": 1,
             "metric": "GLOBAL_RESOURCES",
             "description": "Low free global resource rate",
             "alarmName": "freeGlobalResourcesRateLow",
             "id": 51
            }
         ]
        }

###Retrieve configured alarms [GET]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {

        }

+ Response 200 (application/json)

        {
          "alarms": [
            {
              "scalingPolicyId": null,
              "period": 10,
              "comparisonOperator": "GREATER_THAN",
              "threshold": 10,
              "metric": "PENDING_CONTAINERS",
              "description": "Number of pending containers is high",
              "alarmName": "pendingContainerHigh",
              "id": 50
            },
            {
              "scalingPolicyId": null,
              "period": 10,
              "comparisonOperator": "EQUALS",
              "threshold": 1,
              "metric": "GLOBAL_RESOURCES",
              "description": "Low free global resource rate",
              "alarmName": "freeGlobalResourcesRateLow",
              "id": 51
            }
          ]
        }

###Delete an alarm [DELETE]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.
    + alarmId (required String `alarmId`) ... The id of the alarm.

+ Request (application/json)
        
        {

        }

+ Response 200 (application/json)

        {
          "alarms": [
            {
              "scalingPolicyId": null,
              "period": 10,
              "comparisonOperator": "EQUALS",
              "threshold": 1,
              "metric": "GLOBAL_RESOURCES",
              "description": "Low free global resource rate",
              "alarmName": "freeGlobalResourcesRateLow",
              "id": 51
            }
          ]
        }

#Group Scaling Policy
Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application. 
When scaleing policies are used, the capacity is automatically increased or decreased according to the conditions defined.
Periscope will do the heavy lifting and based on the alarms and the scaling policy linked to them it executes the associated policy.

##Add, remove and show [/clusters/{clusterId}/policies]
###Create a scaling policy. [POST]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {
          "minSize": 2,
          "maxSize": 10,
          "cooldown": 35,
          "scalingPolicies": [
            {
              "name": "downScaleWhenHighResource",
              "adjustmentType": "NODE_COUNT",
              "scalingAdjustment": -2,
              "alarmId": "51"
            },
            {
              "name": "upScaleWhenHighPendingContainers",
              "adjustmentType": "PERCENTAGE",
              "scalingAdjustment": 40,
              "alarmId": "50"
            }
          ]
        }

+ Response 201 (application/json)

        {
          "cooldown": 35,
          "scalingPolicies": [
            {
              "alarmId": 50,
              "scalingAdjustment": 40,
              "adjustmentType": "PERCENTAGE",
              "name": "upScaleWhenHighPendingContainers",
              "id": 101
            },
            {
              "alarmId": 51,
              "scalingAdjustment": -2,
              "adjustmentType": "NODE_COUNT",
              "name": "downScaleWhenHighResource",
              "id": 100
            }
          ],
          "maxSize": 10,
          "minSize": 2
        }

###List the scaling policies. [GET]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {

        }

+ Response 200 (application/json)

        {
          "cooldown": 35,
          "scalingPolicies": [
            {
              "alarmId": 50,
              "scalingAdjustment": 40,
              "adjustmentType": "PERCENTAGE",
              "name": "upScaleWhenHighPendingContainers",
              "id": 101
            },
            {
              "alarmId": 51,
              "scalingAdjustment": -2,
              "adjustmentType": "NODE_COUNT",
              "name": "downScaleWhenHighResource",
              "id": 100
            }
          ],
          "maxSize": 10,
          "minSize": 2
        }

##Delete [/clusters/{clusterId}/policies/{policyId}]
###Delete a scaling policy. [DELETE]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.
    + policyId (required String `policyId`) ... The id of the policy.

+ Request (application/json)

        {

        }

+ Response 200 (application/json)

        {
          "cooldown": 35,
          "scalingPolicies": [
            {
              "alarmId": 50,
              "scalingAdjustment": 40,
              "adjustmentType": "PERCENTAGE",
              "name": "upScaleWhenHighPendingContainers",
              "id": 101
            }
          ],
          "maxSize": 10,
          "minSize": 2
        }

#Group Applications
A Hadoop YARN application is a workload submitted to a cluster. An application requests for resources from YARN Resource Manager. The resources are allocated as YARN containers.

##List [/clusters/{clusterId}/applications]
###List running applications and their resources [GET]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {

        }

+ Response 200 (application/json)

        [
            {
                "usedVCores": 1,
                "usedMemory": 512,
                "reservedContainers": 0,
                "usedContainers": 1,
                "appId": "application_1407836063840_0001",
                "user": "hdfs",
                "queue": "default",
                "state": "ACCEPTED",
                "url": "http://amb0.mycorp.kom:8088/proxy/application_1407836063840_0001/",
                "start": 1407847270776,
                "finish": 0,
                "progress": 0
              }
        ]

#Group Configuration
The CapacityScheduler is designed to run Hadoop applications as a shared, multi-tenant cluster in an operator-friendly manner while maximizing the throughput and the utilization of the cluster.
Periscope facilites the reconfiguration of the CapacityScheduler, used by the application movement/re-prioritization of running applications.

##Update [/clusters/{clusterId}/configurations]
###Reload Hadoop YARN configuration [POST]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {

        }

+ Response 200 (application/json)

        {
            "appMovement": "allowed",
            "state": "RUNNING",
            "port": "8080",
            "host": "172.24.0.2",
            "id": "multi-node"
        }

##Queue setup [/clusters/{clusterId}/configurations/queue]
###Reconfigure the queue capacities [POST]

+ Parameters
    + clusterId (required String `clusterID`) ... The id of the cluster.

+ Request (application/json)

        {
          "setup": [
            {
              "name": "default",
              "capacity": 55
            },
            {
              "name": "high",
              "capacity": 45
            }
          ]
        }

+ Response 200 (application/json)

        {
          "properties": {
            "yarn.scheduler.capacity.root.capacity": "100",
            "yarn.scheduler.capacity.root.high.maximum-capacity": "45",
            "yarn.scheduler.capacity.root.default.user-limit-factor": "1",
            "yarn.scheduler.capacity.root.high.user-limit-factor": "1",
            "yarn.scheduler.capacity.root.default.acl_submit_applications": "*",
            "yarn.scheduler.capacity.maximum-am-resource-percent": "0.2",
            "yarn.scheduler.capacity.root.high.state": "RUNNING",
            "yarn.scheduler.capacity.root.default.maximum-capacity": "55",
            "yarn.scheduler.capacity.node-locality-delay": "40",
            "yarn.scheduler.capacity.root.default.acl_administer_jobs": "*",
            "yarn.scheduler.capacity.root.queues": "default,high",
            "yarn.scheduler.capacity.root.default.state": "RUNNING",
            "yarn.scheduler.capacity.root.unfunded.capacity": "50",
            "yarn.scheduler.capacity.root.high.acl_administer_jobs": "*",
            "yarn.scheduler.capacity.root.acl_administer_queue": "*",
            "yarn.scheduler.capacity.root.high.acl_submit_applications": "*",
            "yarn.scheduler.capacity.root.high.capacity": "45",
            "yarn.scheduler.capacity.maximum-applications": "9999",
            "yarn.scheduler.capacity.root.default.capacity": "55"
          },
          "setup": [
            {
              "capacity": 55,
              "name": "default"
            },
            {
              "capacity": 45,
              "name": "high"
            }
          ]
        }
